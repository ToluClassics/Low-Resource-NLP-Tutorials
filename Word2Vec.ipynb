{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Word2Vec\n",
        "\n",
        "“you shall know a word by the company it keeps” — (Firth, J. R. 1957:11)\n",
        "\n",
        "As the goal of this resource is to teach NLP for low-resource languages, we are going to start with learning word embeddings for some low resource languages e.g Yoruba, Igbo, Swahili e.t.c from scratch using word2vec. Word2Vec was introduced in [Efficient Estimation of Word Representations in\n",
        "Vector Space](https://arxiv.org/pdf/1301.3781.pdf) and the overarching idea is that the meaning of a word is dependent on the context of which it is often used.\n",
        "\n",
        "In this tutorial, we would learn and visualize word embeddings for different languages using the [mC4](https://huggingface.co/datasets/mc4) dataset and pytorch.\n",
        "\n",
        "\n",
        "## Important terminologies to note\n",
        "\n",
        "- CBOW:\n",
        "- SkipGram:\n",
        "- Corpus:\n",
        "- Vocabulary:\n",
        "- Word Subsampling:\n"
      ],
      "metadata": {
        "id": "KcpS4B-Np7Bv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "\n",
        "- First we need to build a corpus which consists of all th"
      ],
      "metadata": {
        "id": "2K9dABeqviIL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIuBRQn5pOT6"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ]
}